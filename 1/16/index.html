<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introducción Python</title>
    <link rel="shortcut icon" type=image/jpg href="../../img/logo/favicon.ico"/>
    <link href="https://fonts.googleapis.com/css2?family=Carrois+Gothic+SC&display=swap" rel="stylesheet"> 
    <link rel="stylesheet" href="css/style.css">

</head>
<body>
    <div class="context">
        <a href="../15/index.html">Volver</a>
        <a href="../../index.html">Inicio</a>
    </div>
    <div class="contenido" id="arriba">
        <p>Python - Notas</p>
    </div>

    <h2>Extras:</h2>
    <h1>1 - Fundamentos de Web Scraping con Python y Xpath</h1>
    <ol>
        <li>
            <a href="#1">1-Qué es el web scraping</a>
        </li>
        <li>
            <a href="#2">2-Por qué aprender web scraping hoy</a>
        </li>
        <li>
            <a href="#3">3-Python- el lenguaje más poderoso para extraer datos</a>
        </li>
        <li>
            <a href="#4">4-Entender HTTP</a>
        </li>
        <li>
            <a href="#5">5-Qué es HTML</a>
        </li>
        <li>
            <a href="#6">6-Robots.txt- permisos y consideraciones al hacer web scraping</a>
        </li>
        <li>
            <a href="#7">7-XML Path Language</a>
        </li>
        <li>
            <a href="#8">8-Tipos de nodos en XPath</a>
        </li>
        <li>
            <a href="#9">9-Expresiones en XPath</a>
        </li>
        <li>
            <a href="#10">10-Predicados en Xpath</a>
        </li>
        <li>
            <a href="#11">11-Operadores en Xpath</a>
        </li>
        <li>
            <a href="#12">12-Wildcards en Xpath</a>
        </li>
        <li>
            <a href="#13">13-In-text search en Xpath</a>
        </li>
        <li>
            <a href="#14">14-XPath Axes</a>
        </li>
        <li>
            <a href="#15">15-Resumen de XPath-Cheatsheet</a>
        </li>
        <li>
            <a href="#16">16-Aplicando lo aprendido</a>
        </li>
        <li>
            <a href="#17">17-Un proyecto para tu portafolio- scraper de noticias</a>
        </li>
        <li>
            <a href="#18">18-Construcción de las expresiones de XPath</a>
        </li>
        <li>
            <a href="#19">19-Obteniendo los links de los artículos con Python</a>
        </li>
        <li>
            <a href="#20">20-Guardando las noticias en archivos de texto</a>
        </li>
        <li>
            <a href="#21">21-Cómo continuar tu ruta de aprendizaje</a>
        </li>
    </ol>

    <p id="1">1-Qué es el web scraping</p>
    <pre>
        Una Manera de extraer información útil de una página web.

        Xpath Para extraer la información de manera efectiva.
        
        El objetivo es realizar nuestros propios scripts para extraer información.
        Realizar un ánalisis de datos con la información extraída. O bien, nutir otra web con la data extraída.
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="2">2-Por qué aprender web scraping hoys</p>
    <pre>
        Está presente en todos lados.
        Las aplicaciones que obtienen diferentes precios para realizar comparaciones como Trivago.
        
        También aplicaciones como ebay para para saber los precios de la competencia.
        --------------------------------------------------------

        Los reclutadores, para ver los perfiles adecuados, para contratar.

        --------------------------------------------------------

        En email marketing, tweets con palabras claves..

        --------------------------------------------------------

        Como Backend o científico de Datos.
        Ya que cuando no hay una API pública para extraer datos, siendo un backend se utiliza el web scrapping.
        Obtener datos.
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="3">3-Python- el lenguaje más poderoso para extraer datos</p>
    <pre>
        Lenguaje con mayor soporte, tiene módulos para realizar web scrapping.
        Màs especializado para hacer cienciad de Datos.

        Como backend se puede usar Django y añadir esta técnica.

        --------------------------------------------------------
        Módulos:
        El más básico es
        Request HTTP		-> Es una librerìa que permite controlar HTTP, conjunto de reglas en el cual se comunican dos computadoras en internet.
                    -> Fundamental

        BeautifulSoup HTML	-> Extraer información a partir de HTML.

        --------------------------------------------------------
        Frameworks Avanzados:
        Selenium.

        Con el cual podemos crear navegadores fantasmas, y controlar sitios web de manera automática.
        De aquí previenen los bots.

        Frameworks más avanzados:
        Scrapy.

        Usado por el Reino unido para obtener los datos de su población.
        Se puede profundizar.

        --------------------------------------------------------
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="4">4-Entender HTTP</p>
    <pre>
        4-Entender HTTP

        Hypertext Transfer Protocol.
        Es un protocolo, un conjunto de reglas con el cual dos computadoras se comunican entre sí.
        --------------------------------------------------------
        
        Por un lado el Cliente, y por otro el Servidor.		Cliente -> Servidor.   Servidor -> Cliente
        
        El cliente realiza la petición y el Servidor Responde.. Esa petición.
        
        --------------------------------------------------------
        #Request	-> Petición Http
        
        GET / HTTPS/1.1					- Método, GET (Traer), versiòn del Protocolo 1.1	
        Host: developer.mozilla.org			- El sitio web en particular.
        Accept-Language: fr				- fr francés, en english
        
        #Responde	-> Respuesta Http
        
        HTTP/1.1 200 OK					- status code, 200 ok			Son las Cabeceras
        Date: Sat, 09 Oct 2010 14:28:02 GMT		- Fecha de la respuesta
        Server: Apache					- respuesta del servidor
        Last-Modified: Tue, 01 Dec 2099 20:18:22 GMT	- 
        ETag: "51142bc1-7449-479b075b2891b"		- cache	
        Accept-Ranges: bytes				- aceptamos los bytes
        Content-Length: 29769				- cantidad de bytes
        Content-Type: text/html				- el cuerpo text/html
        
        < !DOCTYPE html..(here comes the 29769 bytes of the requested web page)
        --------------------------------------------------------
        
        Imagen.. Donde se ubica HTTP de acuerdo a la Web en su completitud.
        Fundamentos de Ingeniería de Software.
        
                        HTTP
        Protocolos debajo..
        IP 	-> Internet Protocol, donde salen la identificación única que referncia a nuestra pc., (IPV4, IPV6)
        TCP	-> Transmision control protocol, protocolo que establece como se transfiera la información a bajo nivel.
        UDP 	-> Parecido al anterior, User Data Protocol
        TLP	-> Transfer Language Security, que tiene que ver con el encriptado de la información, para que un ataque no pueda ver la información enviada.
        DNS	-> Domain Name System, es lo que resuelve un nombre de dominio a una IP.
            -> Al buscar, google.com, el dns se encarga de transformar lo que se busca en una dirección IP, que al final es del servidor.
        
        HTTP, por encima permite transportar..
        HTML	
        CSS	
        JAVASCRIPT	-> Para tratar la interacción entre diferentes partes de una página
        WEB APIS
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="5">5-Qué es HTML</p>
    <pre>

        Hypertext Markup Language
        Lenguaje para definir estructura de una página web.
        --------------------------------------------------------
        
        Apuntamos a HTML para obtener la data de la estructura.
        Etiquetas con atributos.. Los vemos con el inspector del navegador.
        
        --------------------------------------------------------        
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="6">6-Robots.txt- permisos y consideraciones al hacer web scraping</p>
    <pre>
        Se coloca en la raíz de un sitio.. 
        Ejemplo
        platzy.com/robots.txt	
        
        
        User-Agent: *					-> Pequeña claúsula, cuando se identifica una pc al hacer una petición. * Todos, un script 
        Allow: /					-> Permite visitar el directorio completo.
        Allow: /conf/*
        Allow: /conf-og/*		
        Disallow: /*/*/concepto/*/*/material/		-> No permite entrar a rutas en particular.
        Disallow: /login/facebook/			-> No permitido para el resto.
        Disallow: /login/twitter/
        Disallow: /*/*/live/
        Disallow: /*/*/%7B%7Burl%20absolute=/
        Disallow: /*/*/add_contribution/
        Disallow: /mi-suscripcion/
        Disallow: /r/
        Disallow: /clases/*/nuevos_materiales/
        Disallow: /kit-ui/
        Disallow: /ui/
        Disallow: /sfotipy/
        Disallow: /streaming/*
        Disallow: /payments/*
        Disallow: /*/add_review/
        Disallow: /*/save/
        Disallow: /adquirir/*
        Disallow: /comentario/
        Disallow: /comment/
        Disallow: /comprar/
        Disallow: /precios/*/
        Disallow: /yearly-stats-share/
        Disallow: /courses/
        Disallow: /historias/
        Disallow: /becas-fb/
        Disallow: /testimonios/
        
        Sitemap: https://platzi.com/sitemap.xml
        --------------------------------------------------------
        
        También lo usan los motores de bùsqueda para indexar (algunas veces), ciertas páginas de nuestro sitio, y no indexar algunas.
        Google hace constantemente un web scrapping de todas las páginas para mostrar algunas y otras no en el buscador..
        En base al archivo que cada página tiene pre configurada.
        
        --------------------------------------------------------
        
        Al definir este archivo, evitamos que otros clientes, toque información sensible de nuestra pagina web.
        Información que no se acceda, de buenas prácticas.
        Evitar ademàs problemas legales.
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="7">7-XML Path Language</p>
    <pre>
        XPath
        XML Path Language
        
        Similar a Html es XML
        Lenguaje formado por Nodos.
        
        Expresiones Regulares, para encontrar patrones que apunten a coincidencias en un texto.
        
        --------------------------------------------------------
        Xpath es para Html
        Como lo que son..
        Las expresiones regulares, para un texto.
        --------------------------------------------------------
        
        Xpath
        
        //div/span//h1[@class="tittle"][1]		-> Extraer un tìtulo de un span, que está dentro de un div.
                        -> EL primer titulo
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="8">8-Tipos de nodos en XPath</p>
    <pre>
        Un nodo es lo mismo que una etiqueta y su contenido.

        --------------------------------------------------------
        
        toscrape.com		-> Sandbox para realizar prácticas, sin problemas ni preocupaciones.
        
        
        F2, inspector de Elementos.
        Apuntar los Nodos.
        
        Html	-> Head
            -> Body	-> Footer
                -> Div class="container"	-> div class="row"
                                -> div class="row header-box"	-> div class="col-md-8"	-> h1
        
        * Dato a extraer por ejemplo, h1
        * Queremos extraer las citas.
        --------------------------------------------------------
        
        Por lo tanto Xpath, es un lenguaje que permite navegar por los nodos, para extraer información.
        Para usarla en nuestros sitio web o ponerla en nuestro software de analisis de datos.	
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="9">9-Expresiones en XPath</p>
    <pre>
        Primeras expresiones con xPath

        Entramos a console.. Desde el inspector.
        A travès de ciertas instrucciones.
        
        --------------------------------------------------------
        
        $x('/')			-> [documen]		->/ marca el route, apuntamos a todo el documento.
                            -> Tambien puede indicar un salto de nodo, de un nivel a otro / dentro de la expresión.
        
        $x('/html')		-> [html]
        
        $x('//')					-> 			-> Para hacer un salto de nodo, evitando colocar el camino completo /html/body/etc.
        $x('//h1')					-> [h1]			-> Apunta al unico h1
        
        $x('//h1/a/text()').map(x => x.wholeText)	-> [text, text]		-> Obtenemos el texto del titulo, map() es de Javascript
        
        $x('//span/..')					-> Selecciona los div, que estàn por encima del span
        
        $x('//span/.')					-> Seleccion del nodo actual, es igual a $x('//span')		
        
        $x('//span/@class')				-> Trae todas las clases del span que hay en el documento.	
        
        
        --------------------------------------------------------        
    </pre>
    <p id="10">10-Predicados en Xpath</p>
    <pre>
        De esta manera filtraremos la información de una manera màs especìfica, al extraer infromaciòn.	 

        --------------------------------------------------------
        
        $x('/html/body/div/div[1]')				-> Obtenemos el 1ª div de la lista.
        
        $x('/html/body/div/div[last]')				-> Obtenemos el ultimo div de la lista.
            
        $x('//span[@class]')					-> Traemos todos los span que tengan como atributo una clase.
        
        $x('//span[@class="text"]')				-> y con texto..
        
        $x('//span[@class="text"]/text()').map(x => x.wholeText)-> Trae los textos, de todos los nodos span de clase texto.
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="11">11-Operadores en Xpath.</p>

    <pre>
        Filtrar de Manera más específica.	

        --------------------------------------------------------
        
        $x('//span[@class!="text"]/text()').map(x => x.wholeText)	-> != distintos de texto.
        
        
        $x('//span[@class!="text"]')					-> Vemos los nodos, con su clase distinta a texto
        
        $x('/html/body/div/div[psoition()=1]')		-> Div en posicion igual a 1, el primero
        
        $x('/html/body/div/div[psoition()>1]')		-> Todos los Div por encima de ese primer elemento, si es uno, solo uno.
        
        $x('//span[@class="text" and @class="tag-item"]')	-> and, las dos a la vez.
        
        $x('//span[@class="text" or @class="tag-item"]')	-> or, una o la otra.
        
        $x('//span[not(@class)]')				-> not, todos los span que no tengan de atributo el class
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="12">12-Wildcards en Xpath.</p>

    <pre>
        Comodines, usados al no saber el nodo especifico, pero si una aproximación.

        --------------------------------------------------------
        Para traer nodos que no sabemos como se llaman pero si sabemos donde están.
        
        $x('/*')				-> Traemos el html
        
        $x('/html/*')				-> Traemos todos los nodos que están a continuación de Html, head y body, por ejemplo.
        
        $x('//*')				-> Saltamos todos los niveles en todas las direcciones, obtenemos TODO, los nodos y también sus atributos..
                            -> útil para traer todo el documento.
        
        $x('//span[@class="text"]/@*')		-> Traemos todos los atributos, de todos los nodos, de tipo span que tienen como clase a text.
                            -> Podemos traer atributos desconocidos.	
        
        $x('/html/body//div/@*')		-> Traemos todos los atributos de todos los div que están después de body.	
        
        $x('//span[class="text" and @itemprop="text"]/node()')		-> Todos los span que cumplen con las condiciones.
                                        -> node() es parecido a *, pero node trae lo que va más alla de los nodos, el contenido (text)
                                        - con node todo lo que no sea los nodos, pero también los nodos, trae TODO.
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="13">13-In-text search en Xpath</p>

    <pre>
        Siendo aún más específico podemos buscar dentro del texto.

        --------------------------------------------------------
                Traemos todos los autores que comienzan con la letra a.
        $x('//small[@class="author" and starts-with(., "A")]')		-> con (. el punto nos referimos al nodo actual 
        
        $x('//small[@class="author" and starts-with(., "A")]/text()').map(x => x.wholeText)		-> Nos trae los autores.
        
        $x('//small[@class="author" and contains(., "Ro")]/text()').map(x => x.wholeText)		-> Vemos los nombres que tengan Ro
        
        $x('//small[@class="author" and end-with(., "t")]/text()').map(x => x.wholeText)		-> Que terminen
        
        $x('//small[@class="author" and matches(., "A.*n")]/text()').map(x => x.wholeText)		-> Matches con expresiones regulares
                                                        -> Que empiezan con A y terminan con n.
        
        $x('//small[@class="author" and starts-with(., "E")]/text()').map(x => x.wholeText)		
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="14">14-XPath Axes.</p>

    <pre>
        Para traer el nodo padre de un Nodo, y todos los ancestros.. Xpath Axes.
        Indicamos relaciones màs directas.
        
        --------------------------------------------------------
        
        $x('/html/body/div/self::div')			->self::div es igual /., Azúcar sintáctico para representar algo largo de manera màs concreta.
        
        $x('/html/body/div/child::div')			-> Los hijos
        
        $x('/html/body/div/descendant::div')		-> Los hijos como los "Nietos", todos.
        
        $x('/html/body/div/descendant-or-self::div')	-> La union entre los descendentes de ese nodo y el nodo en si mismo.				
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="15">15-Resumen de XPath-Cheatsheet</p>
    <br>
    <img src="img/1 - Xpath  - Expresiones Predicados.PNG" alt="">
    <br>
    <img src="img/2 - Xpath  - Operadores Wildcards.PNG" alt="">
    <br>
    <img src="img/3 - Xpath  - In-text search.PNG" alt="">
    <br>
    <img src="img/4 - Xpath  - Axes.PNG" alt="">
    <br>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="16">16-Aplicando lo aprendido.</p>
    <pre>
        Practicando..
        --------------------------------------------------------
        
        En el Sandbox de WebScrapping.
        En la sección de Books...
        
        
        Extraer los títulos de los libros.
        
        $x('//article[@class="product_pod]/h3/a/"title').map(x => x.value)				-> Traemos todos los títulos de los Libros.
        
        $x('//article[@class="product_pod]/div[@class="product_price"]/p[@class="price_color]/text()').map(x => x.wholeText)
                                                        -> Para extraer el precio de todos los libros.
        
        $x('//div[@class="side_categories"]/ul[@class="nav nav-list"]/li/ul/li/a/text()').map(=> x.wholeText)
                                                        -> Extraemos las categorías.				
        --------------------------------------------------------        
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="17">17-Un proyecto para tu portafolio- scraper de noticias</p>
    <pre>
        Script como un sistema de Archivado.
        Iremos a una página de Noticias, para extraer tanto el título como el cuerpo.
        
        www.larepublica.co/robots.txt
        
        User-agent: Googlebot
        Allow: /
        User-agent: Googlebot
        Disallow: /vista-previa/
        User-agent: Googlebot-News
        Allow: /
        User-agent: Googlebot-News
        Disallow: /vista-previa/
        User-agent: *
        Allow: /
        User-agent: *
        Disallow: /vista-previa/				-> Lugar donde no se puede entrar.
        Disallow: /pagos/
        Sitemap: https://www.larepublica.co/sitemapindex
        Sitemap: https://www.larepublica.co/sitemapnews
        
        --------------------------------------------------------
        La idea es obtener los títulos como también el cuerpo.
        Cmder		-> Emulador de Bash, podemos ejecutar comandos que son compatibles con sistemas Linux.
        
            C:\Users\ThunderXforce\Desktop\fundamentos-scraping-code
        
            -> mkdir larepublica_scraper		-> Creamos una carpeta
            -> ls					-> Listamos el contenido de la carpeta
            -> cd larepublica_scraper\		-> Entramos.
        
            -> git init
            -> py -m venv venv			-> Entorno virtual de python, venn (virtual envolment), para tener las dependencias de manera aislada.
        
        Abrimos la carpeta en Vscode.
        creamos el archivo .gitignore en la raìz	-> Así evitamos llevarnos la carpeta venv
        
        venv/						-> Dentro del archivo, para ignorarlo.
        
            Desde consola
            -> venv\Scripts\activate		-> Activamos el entorno virtual.
        
        --------------------------------------------------------
        Ahora creamos un espacio de trabajo, ordenado..
        save workspace as				-> larepublica_scraper, lo nombramos como la carpeta.
        
            -> Cuando en la consola aparece (venv)	-> El entorno se encuentra activo.
        
            Ahora instalamos las librerìas, request, Lxml (xpath), autopep8 (para formatear el codigo de manera oficial) 	
            (venv) -> pip install request lxml autopep8
        
            -> py -m pip intstall pip --upgrade	-> Actualizamos pip.
        
        --------------------------------------------------------
        Ya tenemos el entorno creado.
        --------------------------------------------------------        
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="18">18-Construcción de las expresiones de XPath.</p>

    <pre>

        Inspeccionar la página a scrappear..
        Las expresiones van variando de acuerdo al formato de la página web.
        
        --------------------------------------------------------
        
        Primero extraeremos los links que nos llevan a la noticia en particular.
        Inspector de Elementos..
        
        $x('//h2[@class="headline"]/a/@href').map(x => x.value)		-> Traemos los links de todas las noticias del sitio.
        
        Esa expresión la guardaremos en un nuevo archivo.. xpath.txt
        Este archivo serà como referencia para crear luego nuestros scripts.
        
        Links = //h2[@class="headline"]/a/@href
        
        --------------------------------------------------------
        
        Ahora entramos a la noticia en particular para observar como está maquetada.
        Necesitamos..
        -Titulo
        $x('//h1[@class="headline"]/a/text()').map(x => x.wholeText)	-> Obtenemos el título
        
        Titulo = //h1[@class="headline"]/a/text()			-> En el archivo.
        
        -Resumen
        $x('//div[@class="lead"]/p/text()').map(x => x.wholeText)	-> Obtenemos el Resumen.
        
        Resumen = //div[@class="lead"]/p/text()
        
        -Cuerpo
        $x('//div[@class="articlesWrapper  "]/p[not(@class)]/text()').map(x => x.wholeText)	-> not(@class) todos los p que no tenga una clase.
                                                    -> Respetar textualmente los nombres de las clases.
        Cuerpo =  //div[@class="articlesWrapper  "]/p[not(@class)]/text()					-> Obtenemos todos los párrafos.
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="19">19-Obteniendo los links de los artículos con Pythonp>
    <pre>
        Primero comenzamos por buscar todos los links..

        --------------------------------------------------------
            -> Entrar al entorno virtual desde consola.
            venv\Scripts\activate				-> Activamos el entorno
        
        Creamos un nuevo archivo de python en la carpeta, en la raíz.. larepublica_scraper\
        
            -scraper.py
        
        import requests			-> 
        import lxml.html as html	-> de lxml obtenemos con el alias el html		-> Entrar al entorno virtual desde consola.
        
        HOME_URL = 'https://www.larepublica.co/'						-> Definimos la ruta de la página.
        
                                                    -> Creamos las constantes con xpath
        XPATH_LINK_TO_ARTICLE = '//h2[@class="headline"]/a/@href'				- Article
        XPATH_TITLE = '//h1[@class="headline"]/a/text()'					- Title
        XPATH_SUMMARY = '//div[@class="lead"]/p/text()'						- Resumen
        XPATH_BODY = '//div[@class="articleWrapper  "]/p[not(@class)]/text()'			- Cuerpo
        
        
        def parse_home():								-> Extraer los links de las noticias.
        
            try:									-> Lo envolvemos dentro de un try except a todo el codigo													- En caso de obtener una respuesta negativa desde el servidor.
                response = requests.get(HOME_URL)					-> Response, traemos el archivo url de la pagina, home_url
                                                -> La respuesta sera el documento html y todo lo que involucra http (cabeceras)
                if response.status_code == 200:						-> Si la respuesta es ok
        
                    home = response.content.decode('utf-8')				-> response.content.. Devuelve el documento html de la respuesta/ decode, 'ñ@
        
                    parsed = html.fromstring(home)					-> Tomamos el contenido de html que ya tenemos en home de manera de texto
                                                - y lo transformamos en un documento especial al que le haremos xpath
                    links_to_notices = parsed.xpath(XPATH_LINK_TO_ARTICLE)		-> Obtenemos una lista sobre la constante que creamos.	
        
                    # print(links_to_notices)						-> Corroboramos.
        
                else:
                    raise ValueError(f'Error: {response.status_code}')			-> Sino, tenemos un error en la respuesta,
                                                status_code, atributo del response, la respuesta.
            except ValueError as ve:							-> Except en caso de Error, value error.
                print(ve)
        
        
        def run():								-> Funcion principal que corre el programa.
            parse_home()
        
        
        if __name__ == '__main__':						-> Definimos el entry point de nuestro archivo de python.
            run()
        --------------------------------------------------------
        
        Corremos el programa desde consola como..
            -> py scraper.py						-> py por como llamamos python en el sistema.
        
            -> Obtenemos los links
        
        
        --------------------------------------------------------        
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="20">20-Guardando las noticias en archivos de texto</p>

    <pre>
        Armamos ahora la lógica para ordenar la información..
        Links..
        Titulo..
        Resumen..	
        Cuerpo..
        --------------------------------------------------------
        
        import requests
        import lxml.html as html
        import os						1º-> Importamos dos nuevas librerìas propias de Python	
        import datetime						2º-> La fecha de Hoy.
        
        HOME_URL = 'https://www.larepublica.co/'
        
        XPATH_LINK_TO_ARTICLE = '//h2[@class="headline"]/a/@href'
        XPATH_TITLE = '//h1[@class="headline"]/a/text()'
        XPATH_SUMMARY = '//div[@class="lead"]/p/text()'
        XPATH_BODY = '//div[@class="articleWrapper  "]/p[not(@class)]/text()'
        
        
        def parse_notice(link, today):						6º -> Creamos la función.. 
            try:								7º -> Envolvemos en try except.
                response = requests.get(link)
                if response.status_code == 200:					9º -> Con el estatus correcto..
                    notice = response.content.decode('utf-8')			11º -> Traemos la respuesta como el punto anterior, como html
                    parsed = html.fromstring(notice)				12ª -> Le pasamos el html
        
                    try:							-> Armamos el formato.
                        title = parsed.xpath(XPATH_TITLE)[0]			13º-> [0], nos devuelve una lista.. Accedemos al title, el 1ª elemento.
                        title = title.replace('\"', '')				15º-> Reemplazamos las comillas dobles por nada..
                                                -Escapamos la comilla doble \"
                        summary = parsed.xpath(XPATH_SUMMARY)[0]		14º-> [0], igual
                        body = parsed.xpath(XPATH_BODY)				-> Como es una lista de párrafos la traemos todo completo.
                    except IndexError:
                        return							-> Puede que algunas noticias no tengan resumenes, por lo cual.. Salimos de la funcion.
        
                        -> with es un manejador contextual, si el archivo se cierra de manera inesperada, mantiene todo de manera segura.
        
                    with open(f'{today}/{title}.txt', 'w', encoding='utf-8') as f:		-> 16º Abrimos el archivo, fecha/titulo.txt
                                                    - w modo escritura, el alias es f
                        f.write(title)					-> alias. titulo.. Es decir en el archivo escribimos la data.
                        f.write('\n\n')					-> El salto de linea \n\n
                        f.write(summary)				- Resumen.
                        f.write('\n\n')
                        for p in body:					-> Cuerpo, con un ciclo, ya que es una lista de parrafos.
                            f.write(p)					P y salto de línea.
                            f.write('\n')
                else:
                    raise ValueError(f'Error: {response.status_code}')			10º -> error, mostramos el status_code, lo muestra con el print.
            except ValueError as ve:							8º-> Error.
                print(ve)
        
        
        def parse_home():
            try:
                response = requests.get(HOME_URL)
                if response.status_code == 200:
                    home = response.content.decode('utf-8')
                    parsed = html.fromstring(home)
                    links_to_notices = parsed.xpath(XPATH_LINK_TO_ARTICLE)
                    # print(links_to_notices)
        
                    today = datetime.date.today().strftime('%d-%m-%Y')			3º-> Configuramos la fecha, la creareamos con el modulo datetime
                                            - Guardamos dentro de today un objeto de tipo fecha, con el formato strftime.
                    if not os.path.isdir(today):				4º -> Creamos la carpeta con el nombre definido.. 
                        os.mkdir(today)						-Si no existe, la creamos.
        
                    for link in links_to_notices:				5º -> Hacemos un ciclo en funcion del link y las fechas.
                        parse_notice(link, today)
                else:
                    raise ValueError(f'Error: {response.status_code}')
            except ValueError as ve:
                print(ve)
        
        
        def run():
            parse_home()
        
        
        if __name__ == '__main__':
            run()
        
        --------------------------------------------------------
        Código COmpleto
        --------------------------------------------------------
        import requests
        import lxml.html as html
        import os
        import datetime
        
        HOME_URL = 'https://www.larepublica.co/'
        
        XPATH_LINK_TO_ARTICLE = '//h2[@class="headline"]/a/@href'
        XPATH_TITLE = '//h1[@class="headline"]/a/text()'
        XPATH_SUMMARY = '//div[@class="lead"]/p/text()'
        XPATH_BODY = '//div[@class="articleWrapper  "]/p[not(@class)]/text()'
        
        
        def parse_notice(link, today):
            try:
                response = requests.get(link)
                if response.status_code == 200:
                    notice = response.content.decode('utf-8')
                    parsed = html.fromstring(notice)
        
                    try:
                        title = parsed.xpath(XPATH_TITLE)[0]
                        title = title.replace('\"', '')
                        summary = parsed.xpath(XPATH_SUMMARY)[0]
                        body = parsed.xpath(XPATH_BODY)
                    except IndexError:
                        return
        
                    with open(f'{today}/{title}.txt', 'w', encoding='utf-8') as f:
                        f.write(title)
                        f.write('\n\n')
                        f.write(summary)
                        f.write('\n\n')
                        for p in body:
                            f.write(p)
                            f.write('\n')
                else:
                    raise ValueError(f'Error: {response.status_code}')
            except ValueError as ve:
                print(ve)
        
        
        def parse_home():
            try:
                response = requests.get(HOME_URL)
                if response.status_code == 200:
                    home = response.content.decode('utf-8')
                    parsed = html.fromstring(home)
                    links_to_notices = parsed.xpath(XPATH_LINK_TO_ARTICLE)
                    # print(links_to_notices)
        
                    today = datetime.date.today().strftime('%d-%m-%Y')
                    if not os.path.isdir(today):
                        os.mkdir(today)
        
                    for link in links_to_notices:
                        parse_notice(link, today)
                else:
                    raise ValueError(f'Error: {response.status_code}')
            except ValueError as ve:
                print(ve)
        
        
        def run():
            parse_home()
        
        
        if __name__ == '__main__':
            run()
        --------------------------------------------------------        
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="21">21-Cómo continuar tu ruta de aprendizaje</p>
    <pre>
        Recomendaciones éticas..
        Revisar el archivo Robots.txt
        Del sitio al que le hacemos scrapping.
        --------------------------------------------------------
        
        Considerar la ley de datos en el país en el cual estamos.
        
        --------------------------------------------------------
        
        Si hay que pagar y extraemos datos, no hacerlo.
        
        --------------------------------------------------------        
    </pre>

    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <br>
    <br>

    
    <div class="context">
        <a href="../15/index.html">Volver</a>
        <a href="../../index.html">Inicio</a>
    </div>

    <script src="js/01.js"></script>

</body>
</html>