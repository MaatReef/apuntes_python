<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introducción Python</title>
    <link rel="shortcut icon" type=image/jpg href="../../img/logo/favicon.ico"/>
    <link href="https://fonts.googleapis.com/css2?family=Carrois+Gothic+SC&display=swap" rel="stylesheet"> 
    <link rel="stylesheet" href="../../css/stylesBlue.css">

</head>
<body>
    <div class="context">
        <a href="../14/index.html">Volver</a>
        <a href="../../index.html">Inicio</a>
        <a href="../16/index.html">Siguiente</a>
    </div>
    <div class="contenido" id="arriba">
        <p>Python - Notas</p>
    </div>

    <h1>15 - Programa un Modelo de Machine Learning</h1>
    <ol>
        <li>
            <a href="#1">1. Meta del Día 15</a>
        </li>
        <li>
            <a href="#2">2. Conoce a Google Colab</a>
        </li>
        <li>
            <a href="#3">3. Numpy - Parte 1</a>
        </li>
        <li>
            <a href="#4">4. Numpy - Parte 2</a>
        </li>
        <li>
            <a href="#5">5. Pandas - Parte 1</a>
        </li>
        <li>
            <a href="#6">6. Pandas - Parte 2</a>
        </li>
        <li>
            <a href="#7">7. Pandas - Parte 3</a>
        </li>
        <li>
            <a href="#8">8. Matplotlib - Parte 1</a>
        </li>
        <li>
            <a href="#9">9. Matplotlib - Parte 2</a>
        </li>
        <li>
            <a href="#10">10. Introducción a Machine Learning</a>
        </li>
        <li>
            <a href="#11">11. Sobrevivir al Titanic</a>
        </li>
    </ol>

    <p id="1">1. Meta del Día 15</p>
    <pre>
        Data Science -> Ciencia del estudio de los datos.
        Veremos solo como aplicar python a la ciencia de datos.
        - Numpy		-> Operaciones mátemáticas.
        - Pandas	-> Analizar datasets (Cojuntos de datos, filas, columnas..). El excel de Python.
        - Matplotlib	-> Visualizar Datos.
        
        --------------------------------------------------------
        Construiremos un Machine Learning -> Aprendizaje de la Máquina..
        Entrenar la Pc en base a su expriencia con los datos previstos, pueda predecir el resultado de siguientes situaciones.
        
        Consiste en un módulo que revisará la base de datos de pasajeros del Titanic.
        En base a quienes sobrevivieron y quiénes no, podrá predecir, cuales son las mejores características para sobrevivir a su hundimiento.
        Incluso, teniendo nuevo pasajeros sabremos si sobrevivirán o no.
        --------------------------------------------------------
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>

    <p id="2">2. Conoce a Google Colab</p>
    <pre>
        -> Mis colabs	
            -> Desde Drive, más		
                -> Google Colaboratory
                    -> Para compartir archivos desde ahí, es súper útil. 

        *Considerar el posicionamiento de las celdas en google colab, son independientes*
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    
    <p id="3">3. Numpy - Parte 1</p>
    <pre>
        Biblioteca para realizar cálculos matemáticos a gran escala, ciencia de Datos.
        <a href="https://colab.research.google.com/drive/1vp7zrchG_pJF3uzEgbCjfb_piu43mJXS?usp=sharing#scrollTo=9fwuO-3_viZM">Ver</a>
        <a href="https://numpy.org/devdocs/user/index.html">Libreria oficial</a>
        --------------------------------------------------------
        
        #Importamos Numpy con su abreviación "np"
        import numpy as np
        
        --------------------------------------------------------
        
        #Podemos crear Arrays de una dimensión con la función np.array()
        array_unidim = np.array([1,2,3,4,5])
        
        #O un array de dos dimensiones (bidimensional)
        array_bidim = np.array([[1,2,3], [4,5,6]])
        
        #O un array de tres dimensiones (Tridimiensional)
        array_tridim = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]])
        
        --------------------------------------------------------
        # Para cada uno de estos arrays, podemos obtener sus propiedades, tales como su "forma", número de dimensiones, tipos de datos y tamaño.
        
        
        # Atributos del array unidimensional (forma, número de dimensiones, tipos de datos, tamaño, y tipo)
        array_unidim.shape, array_unidim.ndim, array_unidim.dtype, array_unidim.size, type(array_unidim)
                
                        -> ((5,), 1, dtype('int64'), 5, numpy.ndarray)
                        
        
        
        # Atributos del array bidimensional
        array_bidim.shape, array_bidim.ndim, array_bidim.dtype, array_bidim.size, type(array_bidim)
        
                        -> ((2,3), 2, dtype('int64'), 6, numpy.ndarray)
        
        
        # Atributos del array tridimensional
        array_tridim.shape, array_tridim.ndim, array_tridim.dtype, array_tridim.size, type(array_tridim)
        
                        -> ((2,2,3), 3, dtype('int64'), 12, numpy.ndarray)
        
        
        # Importamos pandas como pd, y creamos un DataFrame a partir del array bidimensional
                -> Pandas para analizar datos en python
        
        import pandas as pd
        
        datos = pd.DataFrame(array_bidim)		-> print(datos)	-> Una Matriz
        
        
        # Creamos un array de tamaño 4x3, formado únicamente por unos (1)
        
        unos = np.ones((4, 3))
        
                            
        # Creamos un array de tamaño 2x4x3, formado únicamente por ceros (0)
        
        cero = np.zero((2, 4, 3))		-> Array de dos dimensiones de profundidad, 4 de alto y 3 de ancho
        
        
        # Creamos un array de números en el rango de 0 a 100, con un paso de 5
        
        array_1 = np.arange(0, 100, 5)		-> Sin el 100 incluido.
        
        
        # Creamos un array de números aleatorios enteros comprendidos en entre 0 y 10, de tamaño (2, 5)
                -> random ya vien en numpy
                
        array_2 = np.random.randint(0, 10, (2, 5))			-> Array de números aleatorios
        
        
        # Creamos un array de números aleatorios decimales comprendidos en entre 0 y 1, de tamaño (3, 5)
        
        array_3 = np.random.random((3, 5))			-> Array de 3x5 aleatorio, 
        
        
        # Establecemos la "semilla" de números aleatorios en 27
        
        np.random.seed(27)
        
        
        # Creamos un array de números aleatorios enteros comprendidos en entre 0 y 10, de tamaño (3, 5)
        
        array_4 = np.random.randint(0, 10, (3,5))			-> "Aleatorios", ya que la semilla en este caso será 27, establecida antes.
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <p id="4">4. Numpy - Parte 2</p>
    <pre>
        Continuando..
        --------------------------------------------------------
        ¿Qué ocurre al correr la última celda nuevamente, a diferencia de las anteriores?
        
        # Encontramos los valores únicos del array_4
        
        np.unique(array_4)				-> Encontrar unique sin repeticiones.
        
        
        
        # Extramos el elemento de índice 1 del array_4
        
        array_4[1]
        
        
        
        # Extraemos las primeras dos filas del array_4
        
        array_4[:2]
        
        
        # Extraemos los dos primeros datos de las primeras dos filas del array_4
        
        array_4[:2, :2]
        
        
        # Creamos dos arrays de tamaño 3x4: uno relleno de números aleatorios entre 0 y 10, y otro relleno de unos
        
        array_5 = np.random.randint(0, 10, (3,4))
        array_6 = np.ones((3, 4))
        
        
        # invocamos el array_5
        
        array_5
        
        # invocamos el array_6
        
        array_6
        
        # Sumamos los dos arrays
        
        array_4 + array_6		-> Sumamos en base a los indices.
        
        
        # Creamos ahora un array de tamaño (4,3) lleno de unos
        
        array_7 = np.ones((4,3))
        
        
        # Intentaremos sumar los arrays 6 y 7
        
        array_6 + array_7			-> Error, los array se suman si tienen iguales dimensiones.
                            * En google colab te envía directamente a stackoverflow*
        
        --------------------------------------------------------
        ¿A qué se debe el error anterior? ¿Qué deberíamos tener en cuenta para que no suceda?
        - Los array deben tener la misma dimensión para poder sumarlos correctamente.
        
        
        # Entonces crearemos otro array de tamaño (4,3) lleno de unos
        
        array_8 = np.ones((4, 3))
        
        
        # Restamos el array_8 al array_7
        
        array_8 - array_7				-> Se puede mientras tengan igual dimensión.
        
        
        # Creamos otros dos arrays de tamaño 3x3 con números aleatorios del 1 al 5
        
        array_9 = np.random.randint(1,5,(3,3))
        array_10 = np.random.randint(1,5,(3,3))
        
        
        # invocamos el array_9
        
        array_9 
        
        
        # invocamos el array_10
        
        array_10
        
        
        # Multiplicamos los últimos dos arrays entre sí
        
        array_9 * array_10			-> Se puede mientras tengan la misma dimensión
        
        
        # Elevamos el array_9 al cuadrado
        
        array_9**2				-> Potenciar.
        
        
        # Buscamos la raíz cuadrada del array_10
        
        np.sqrt(array_10)			-> Raiz cuadarada de cada uno de los numeros del array.
        
        
        # Hallamos el promedio de los valores del array_9
        
        array_9.mean()			-> Promedio
        
        
        # Hallamos el valor máximo de los valores del array_9
        
        array_9.max()			-> El máximo
        
        
        # Hallamos el valor mínimo de los valores del array_9
        
        array_9.min()
        
        
        # Cambiamos la forma del array_9 por una de 9x1, y lo almacenamos como array_11
        
        array_11 = arra_9.reshape((9,1))
        
        
        # invocamos el array_11
        
        
        array_11			-> Bidimensional más alto que ancho.
        
        # Transponemos el array_11
        
        
        array_11.T			-> Tranponerlo, mas ancho que alto.
        
        
        # Comparamos el array_9 y el array_10, para saber cuáles elementos del array_9 son mayores a los del array_10
        
        array_12 = array_9 > array_10 
        
        
        --------------------------------------------------------
        ¿Qué tipos de datos forman parte del array de resultados?
        # Veamos sus nuevos tipos de datos
        
        array_12.dtype		-> dtype('bool')
        
        
        # Alguno de los elementos del array_9 es igual su equivalente del array_10?
        
        array_9 == arra_10
        
        
        # Comparamos nuevamente ambos arrays, en esta ocasión con >=
        
        array_9 >= array_10
        
        # Buscamos los elementos del array_9 que son mayores a 2
        
        array_9 > 2
        
        # Ordenamos de menor a mayor los elementos dentro del array_9
        
        np.sort(array_9)        
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>

    <p id="5">5. Pandas - Parte 1</p>
    <pre>
        Segundo pilar de la ciencia de Datos, Pandas está montado sobre Numpy
        Librería pandas(El "Excel" de Python)
        <a href="https://colab.research.google.com/drive/1-E33EMCehgPnmqgwm13-SZSnVYOHMpuQ?usp=sharing">Ver</a>
        <a href="https://pandas.pydata.org/pandas-docs/stable/">Libreria oficial</a>

        --------------------------------------------------------

        # Importamos Pandas
        import pandas as pd			-> as nombre abreviado


        # Creamos una serie de números y hallamos su media
        numeros = pd.Series([1,2,3,4,67,35,235,62])
        numeros.mean()						-> Promedio de la serie.


        # Hallamos la suma de dichos números
        numeros.sum()


        # Creamos una SERIE de tres colores diferentes
        colores = pd.Series(['rojo', 'amarillo', 'verde'])


        # Visualizamos la serie creada

        colores				-> Mini tabla con los datos especificados.


        # Creamos una serie con tipos de autos, y la visualizamos

        tipos_autos = pd.Series(['sedan', 'SUV', 'Pick Up'])		-> Al llamarlo tendremos una mini tabla


        # Combinamos las series de tipos de autos y colores en un DATAFRAME

        tabla_autos = pd.DataFrame({'Tipo de Auto': tipo_autos, "Color': colores})	-> Al llamarlo ordenamos una tabla.

        *Con Pandas permite conectar con bases de Datos más grandes*        
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>

    <p id="6">6. Pandas - Parte 2</p>
    <pre>
        --------------------------------------------------------
        # Conectamos el cuaderno actual con nuestro Drive

        from google.colab import drive
        frive.mount('/content/drive')				-> Nos da un link para entrar al google drive.g
                                    - accedemos con el código dispuesto
                                    - Y conectamos

        # Importar "ventas-autos.csv" y convertirlo en un nuevo DATAFRAME

        ventas_autos = pd.read_csv('/Conetent/drive/MyDrive/Colab Notebooks/ventas-arutos.csv')
        ventas_autos											-> Vemos el dataset.

        --------------------------------------------------------
        Este será nuestro "Dataframe de Flujo Vehicular"

        # Exportar el Dataframe como un archivo CSV a mi carpeta "/content/drive/MyDrive/Colab Notebooks/pruebas/"

        ventas_autos.to_csv("/content/drive/MyDrive/Colab Notebooks/pruebas/")
                                            -> Lo transformamos y lo guardamos.

        # Analicemos los tipos de datos disponibles en el dataset de ventas autos

        ventas_autos.dtypes			-> Vemos el tipo de datos, en cada una de las columnas.


        # Apliquemos estadística descriptiva (cantidad de valores, media, desviación estándar, valores mínimos y máximos, cuartiles) al dataset

        ventas_autos.describe()			


        # Obtenemos información del dataset utilizando info()

        ventas_autos.info()						-> Vemos la info.

        # Listamos los nombres de las columnas de nuestro dataset

        ventas_autos.columns						-> Detalles

        # Averiguamos el "largo" de nuestro dataset


        len(ventas_autos)						-> 10 items


        # Mostramos las primeras 5 filas del dataset

        ventas_autos.head()

        # Mostramos las primeras 7 filas del dataset

        ventas_autos.head(7)					-> Las primeras 7 de la cabeza.


        # Mostramos las últimas 5 filas del dataset

        ventas_autos.tail(5)					-> Las ultimas 5

        ventas_autos.tail(3)					-> Las ultimas 5 de la cola



        # Utilizamos .loc para seleccionar la fila de índice 3 del DataFrame

        ventas_autos.loc[3]			-> Los datos de cada columna.


        # Utilizamos .iloc para seleccionar las filas 3, 7 y 9

        ventas_autos.iloc[[3,7,9]]

        --------------------------------------------------------
        :En la documentación podrás observar la diferencia entre el funcionamiento de .loc e .iloc.

        <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html">.loc</a>
        <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html">.iloc</a>

        --------------------------------------------------------

        # Seleccionar la columna "Kilometraje"
        ventas_autos['Kilometraje']				-> Los valores en particular


        # Encontrar el valor medio de la columnas "Kilometraje"
        ventas_autos["Kilometraje"].mean()			-> Promedio

        # Seleccionar aquellas columnas que tengan valores superiores a 100,000 kilómetros en la columna Kilometraje
        ventas_autos[ventas_autos["Kilometraje"] > 100000]


        # Creamos una tabla cruzada de doble entrada entre Fabricante y cantidad de puertas
        pd.crosstab(ventas_autos['Fabricante'], ventas_autos["Puertas"])			


        # Agrupamos las columnas por fabricante y buscandos el valor medio de las columnas numéricas
        ventas_autos.groupby(["Fabricante"]).mean()        
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>

    <p id="7">7. Pandas - Parte 3</p>
    <pre>
        Ahora conectamos Pandas con Matplotlib		-> Gráficos
        --------------------------------------------------------
        # Importamos Matplotlib y creamos un gráfico con los valores de la columna Kilometraje
        import matplotlib as plt
        %matplotlib inline				-> Para darle render a los gráficos.

        ventas_autos['Kilometraje'].plot()		-> Vemos el gráfico visual.



        # Puede que un gráfico más apropiado en este caso sea un histograma?
        ventas_autos["Kilometraje"].hist()		-> Otro gráfico


        # Intentamos graficar la columna de precios
        ventas_autos["Precios (USD)"].plot()		-> Error, hay que eliminar la puntuación de la columna de precios.

        <a href="https://stackoverflow.com/questions/44469313/price-column-object-to-int-in-pandas">Convertir</a>


        # Elimina la puntuación de la columna de precios
        ventas_autos["Precio (USD)"] = ventas_autos["Precio (USD)"].str.replace("[\,\$\.]","")
        ventas_autos["Precio (USD)"] = ventas_autos["Precio (USD)"].astype(int)/100

        ventas_autos["Precio (USD)"].plot()							-> Precios aplicado a los precios.	
                
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    
    <p id="8">8. Matplotlib - Parte 1</p>
    <pre>
        útil para generar gráficos..
        <a href="https://matplotlib.org/stable/index.html"> Ver Documentación </a>
        Además de buscar en google, plt.subplots(), puedes buscar directamente en Google plt.subplots()
        
        --------------------------------------------------------
        # Importamos el módulo de Matplotlib como plt
        import matplotlib.pyplot as plt
        
        # La siguiente línea nos permite ver los gráficos directamente al ejecutarlos en el Notebook.
        %matplotlib inline		-> En el cuaderno de colab
        
        
        # Creamos un gráfico utilizando plt.plot()
        plt.plot()					-> Figura vacía
        
        
        
        # Graficomos una lista de números
        a = [1,5,3,8,7,15]
        plt.plot(a)				-> Representamos gráficamente una gráfica lineal.
        
        
        
        # Creamos dos listas, x e y. Llenamos a la lista x de valores del 1 al 100. 
        x = list(range(101))
        
        # Los valores de y van a equivaler al cuadrado del respectivo valor en x con el mísmo índice
        y = []
        for numero in x:
            y.append(numero**2)			-> Al **2
        
        # Graficamos ambas listas creadas
        plt.plot(x, y)					-> Hacemos una curva exponencial
        
        --------------------------------------------------------
        Hay otra manera de crear gráficos en Matplotlib, utilizando el método orientado a objetos (OO).
        --------------------------------------------------------
        # Creamos el gráfico utilizando plt.subplots()
        fig, ax = plt.subplot()					-> Por convención, fig es toda la figura (el canvas)
                                               ax es el eje
        ax.plot(x, y)						-> Mismo gráfico.
         
        --------------------------------------------------------
        Veamos cómo sería un flujo de trabajo en Matplotlib
        --------------------------------------------------------
        # Importar y preparar la librería
        import matplotlib.pyplot as plt
        €matplotlib inline
        
        # Preparar los datos
        x = list(range(101))
        y = [
        for numero in x:
            y.append(numero**2)]
        
        # Preparamos el área del gráfico (fig) y el gráfico en sí (ax) utilizando plt.subplots()
        fig, ax = plt.subplots()
        
        # Añadimos los datos al gráfico
        ax.plot(x, y)
        
        # Personalizamos el gráfico añadiendo título al gráfico y a los ejes x e y
        ax.set(title= "Graficos de Casos de COVID-19 en Latam", xlabel= "dias", ylabel="casos confirmados")
        
        # Guardamos nuestro gráfico empleando fig.savefig()
        fig.savefig("/ejemplo-grafico-covid.png")
        
        
        --------------------------------------------------------
        Veamos ahora un gráfico de dispersión:
        
        # Creamos un nuveo set de datos utilizando la librería Numpy
        
        import numpy as np
        x_1 = np.linspace(0, 100, 20)
        y_1 = x_1**2
        
        
        # Creamos el gráfico de dispersión de x vs y
        fig, ax = plt.subplots()
        ax.scatter(x_1, y_1)					-> Gráfico con 20 puntos.
        
        
        # Visualizamos ahora la función seno, utilizando np.sin(X)
        fig, ax = plt.subplots()
        
        x_2 = np.linspace(-10, 10, 100)
        y_2 = np.sin(x_2)
        
        ax.scatter(x_2, y_2)				-> Onda senoidal.        
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    
    <p id="9">9. Matplotlib - Parte 2</p>
    <pre>
        --------------------------------------------------------
        Veamos ahora otro tipo de gráfico. Por ejemplo, un gráfico de barras, que por lo general asocia resultados numéricos a variables categóricas (categorías):
        
        # Creemos un diccionario con tres platos y su respectivo precio.
        # Las claves del diccionario serán los nombres de las comidas, y los valores asociados, su precio
        
        comidas = {"lasagna":250, "sopa":150, "roast beef":650}
        
        # Crearemos un gráfico de barras donde el eje x está formado por las claves del diccionario,
        # y el eje y contiene los valores.
        
        fig, ax = plt.sub`lots()
        ax.bar(comidas.keys(), comidas.values())
        
        # Añadimos los títulos correspondientes
        ax.set(title="Precios de comidas", xlabel="comidas", ylabel="Precios")		-> Gráfico de Barras, vertical
        
        # Probemos a continuación con un gráfico de barras horizontales
        fig, ax = plt.subplots()
        ax.barh(list(comidas.keys()), list(comidas.values()))
        ax.set(title="Precios de comidas", ylabel="comidas", xlabel="Precios")		-> Grafico horizontal.
        
        
        --------------------------------------------------------
        Un gráfico semejante es un histograma. Podemos generar números aleatorios que siguen una distribución normal (que se acumulan en torno a un valor central), con la función randn:
        
        # Creamos una distribución de 1000 valores aleatorios distribuidos normalmente
        x = np.random.randn(1000)
        
        
        # Creamos el histograma
        fig, ax = plt.subplots()
        ax.hist(x)			-> Representación de números aleatorios, las ax representan los ejes.
        
        --------------------------------------------------------
        Veamos ahora un caso más complejo, trabajando con subplots, o figuras que cotienen varios gráficos:
        
        # Creamos una figura con 4 subgráficos (2 por fila)
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,5))			-> Creamos 4 gráficos, ancho de 10, alto de 5
        
        --------------------------------------------------------
        Añadimos datos a cada uno de los gráficos (axes)
        
        # Creamos la misma disposición de gráficos, con un tamaño de figura de 10x5
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,5))			
        
        
        # Para nuestro primer gráfico, tomamos el conjunto x_1, y_1, y generamos un gráfico de líneas
        ax1.plot(x_1, y_1)
        
        
        # Para nuestro segundo gráfico, tomamos el conjunto x_2, y_2, y generamos un gráfico de dispersión
        ax2.scatter(x_2, y_2)
        
        # Creamos un gráfico con los precios de tres comidas en la esquina inferior izquierda
        ax3.bar(comidas.keys(), comidas.values())
        
        # El gráfico de la esquina inferior derecha será un histograma de valores aleatorios con distribución normal
        ax4.hist(np.random.randn(1000))
        
                                -> En un solo gráfico creamos 4 distintos, con estilos predeterminados.
        
        --------------------------------------------------------
        Matplotlib tiene un conjunto de varios estilos disponibles, podemos verificarlos de la siguiente manera:
        
        # Verificamos estilos disponibles
        plt.style.available 	
                    -> Todos los estilos.
        
        
        # Cambiamos el estilo predeterminado por "seaborn-whitegrid"
        
        plt.style.use('seaborn-whitegrid')				-> Le aplicamos la cuadrícula de fondo por defecto, a todos los gráficos.
        
        --------------------------------------------------------
        
        Habiendo cambiado el estilo (el cambio más evidente que veremos será una grilla en el fondo de cada gráfico), cambiaremos también los colores de las líneas, puntos y barras en cada uno de los gráficos por códigos hex a nuestra preferencia:
        
        
        # Copiamos los valores de los gráficos anteriores
        # Creamos la misma disposición de gráficos, con un tamaño de figura de 10x5
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,5))			
        
        
        # Para nuestro primer gráfico, tomamos el conjunto x_1, y_1, y generamos un gráfico de líneas
        ax1.plot(x_1, y_1, color="#fcba03")
        
        
        # Para nuestro segundo gráfico, tomamos el conjunto x_2, y_2, y generamos un gráfico de dispersión
        ax2.scatter(x_2, y_, color="#fcba03")
        
        # Creamos un gráfico con los precios de tres comidas en la esquina inferior izquierda
        ax3.bar(comidas.keys(), comidas.values(), color="#03c6fc")
        
        # El gráfico de la esquina inferior derecha será un histograma de valores aleatorios con distribución normal
        ax4.hist(np.random.randn(1000), color="#fc036b")
        
                                    -> Queda con otro estilo, otros colores.. Con una cuadrícula de fondo.     
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>

    <p id="10">10. Introducción a Machine Learning</p>
    <pre>
        Machine Learning -> Rama de la Inteligencia Artificial.. La pc aprende a base de su propia Experiencia

        Para construir este tipo de algoritmo requiere de un nivel de abstracción muy alto..
        - Comprender Texto.
        - Relacionar Conceptos.
        - Buscar patrones.
        
        Aprendizaje Supervisado.. Con una base de datos.
        Buscaremos predecir un patrón al momento de que aparezca un nuevo registro, de un valor determinado de la base de datos. 
        
        
        
        sci-kit learn		-> Veremos esa biblioteca.
        
        Partimos desde un árbol de decisiones.. Es una estructura compuesta de ramas, nodos y hojas.
        Comienza con una nueva instancia que se va desplazando a través de sus ramas..
        A medida que se responden preguntas en cada nodo, de acuerdo a sus características, puede llegar a un hoja que lo define con una etiqueta.
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    
    <p id="11">11. Sobrevivir al Titanic</p>
    <pre>
        Veremos una aproximación a la ciencia de Datos..
        Considerar otros ejemplos, con la base de datos de clientes, teniendo en cuenta la edad, donde viven, el nivel educacional, su nivel económico.
        De esta manera sabremos a que público apuntar si disponemos de la información.
        
        <a href="https://colab.research.google.com/drive/1zRVPpLLmhMkmhX_kB9qrWUppHzRTJPFM?usp=sharing">Ver el colab</a>
        --------------------------------------------------------
        Python TOTAL - Machine Learning
        
        Primer Modelo de ML: Árboles de decisión
        
        En este notebook comenzamos a trabajar en los problemas de Clasificación, una de las tareas más importantes dentro de Machine Learning (dentro, a su vez, de lo que llamamos Aprendizaje Supervisado). Clasificación en Machine Learning consiste en aprender etiquetas discretas "y" a partir de un conjunto de features "X" (que pueden ser uno, dos, o muchos más) tomando como muestra un conjunto de instancias.
        
        En este notebook trabajaremos con uno de los modelos fundamentales de Machine Learning: Árboles de Decisión. Para ello, usaremos el dataset de Titanic y la librería Scikit-Learn. Debido a la implementación orientada a objetos de Scikit-Learn, todos los modelos se entrenan y se usan de la misma forma.
        
        Recuerda que todas las librerías fueron desarrolladas por personas, que en búsqueda de resolver una necesidad de cómputo, escribieron el código que hoy podemos reutilizar para poder poner directamente manos a la obra en lugar de tener que desarrollarlo y optimizarlo una y otra vez. Sin embargo, la idea que quiero transmitirte es que puedes hacer el intento de escribir tú mismo las funciones y clases que importaremos de las librerías. Si bien no es lo habitual y desde luego consumirá mucho más tiempo, este trabajo te permitirá comprender nuevos detalles acerca de lo que estás haciendo.
        
        El dataset de Titanic es famoso entre los estudiantes de Data Science. El mismo ha surgido de una competencia en el sitio Kaggle: Machine Learning from Disaster <a href="https://www.kaggle.com/c/titanic">Ver Sitio Oficial</a>. Veremos una implementación muy sencilla acerca de un posible abordaje para resolverlo, partiendo también de una versión simplificada y filtrada del dataset original de dicha competencia.
        
        Nuestro Dataset está compuesto por una serie de columnas, que tienen los siguientes significados:
        
            Sobreviviente: 0 = No; 1 = Si
            Clase: 1 = Primera Clase; 2 = Segunda Clase; 3 = Tercera Clase
            Género: 0 = Hombre; 1 = Mujer
            Edad: edad en años
            HermEsp: cantidad de hermanos o esposos a bordo del Titanic, para el pasajero en cuestión
            PadHij: cantidad de padres o hijos a bordo del Titanic, para el pasajero en cuestión
        
        Ejercicio: Carga el dataset de Titanic y tomate un rato para estudiar sus características.
        
        [ ]
        # importar bibliotecas
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix
        from sklearn import tree
        
        
        [ ]
        # Vincular disco del drive, el data set
        from google.colab import drive
        drive.mount('/content/drive')
                                -> Obtenemos el código para acceder a la cuenta.
        
        [ ]
        # leer archivo csv "/content/drive/MyDrive/Colab Notebooks/DataSet_Titanic.csv"
        df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/DataSet_Titanic.csv")
        
        [ ]
        # visualizar las primeras 5 filas
        df.head(5)				-> Vemos las primeras 5
        
        Árbol de decisión: como primera aproximación, diremos que es un objeto que, dadas varias instancias con un determinado grupo de features X y unas determinadas etiquetas objetivo y, el árbol de desición aprende automáticamente reglas (de mayor a menor importancia) sobre cada feature de manera de poder decidir qué etiqueta le corresponde a cada instancia.
        
        Vamos a separar el dataset de Titanic en una variable X los atributos que usarás para predecir, y en una variable y la etiqueta que quieres predecir. En este caso, si sobrevivió o no.
        
        [ ]
        # guardar en variable X los atributos predictores (todas las cametiquetas excepto "Sobreviviente")
        X = df.drop('Sobreviviente', axis=1)			-> axis es una columna, si es 0, es una fila
        
        # guardar en y la etiqueta a predecir ("Sobreviviente")
        y = df.Sobreviviente						-> La que quitamos anteriormente la captamos acá.
        
        [ ]
        # visualizar x
        X.head()		-> Los demás datos menos la de Sobreviviente
        
        [ ]
        # visualizar y
        y.head()		-> Sobreviviente
        
        Si queremos entrenar un árbol de decisión para clasificar nuestras instancias, primero debemos crear un objeto correspondiente al modelo. Este objeto será de de la clase DecisionTreeClassifier, la cual importamos desde la librería Scikit-Learn.
        
        [ ]
        # Creamos un objeto arbol
        arbol = DecisionTreeClassifier(max_dept=2, random_state=42)		-> max_depth=3, sera un arbol de tres niveles.
        
        
        Hasta ahora, lo único que hicimos fue crear el objeto, nada más.
        
        Una vez que nuestro modelo fue creado, precisamos entrenarlo sobre nuestros datos. Esto lo logramos con el método fit(...) que poseen todas las clases correspondientes a modelos de Scikit-Learn.
        
        [ ]
        # entrenamos a la máquina
        arbol.fit(X, y)
        
        
        ¿Qué ocurrió?
        
        El modelo ya está entrenado. Esto significa que contamos con una herramienta que, dadas ciertas características de una instancia, nos devuelve qué etiqueta y que el modelo cree que le corresponde. Esto lo podemos hacer utilizando el método predict(...), que también poseen todas las clases correspondientes a modelos de Scikit-Learn.
        
        Nos podríamos preguntar luego: ¿cuál es el porcentaje de instancias bien clasificadas por el modelo? Para responder esto usaremos nuevamente el método predict sobre todo el dataset X. Luego con la función accuracy_score podemos calcular el porcentaje de aciertos que obtenemos al comparar nuestra predicción y_pred contra la clase original y. Recomendamos mirar la documentación de esta función, por ahora simplemente diremos que es una de las tantas métricas que utilizamos para evaluar nuestros modelos, y lo que hace es devolvernos un porcentaje de aciertos.
        
        
        [ ]
        # Predecimos sobre nuestro set
        pred_y = arbol.predict(X)
        
        
        # Comaparamos con las etiquetas reales
        print('Precision: ', accuracy_score(pred_y, y))		-> Precision: 0.802521, (lo mejor sería de 0.85 al 100(imposible), al depurar el dataframe)
        
        
        Esto quiere decir que el clasificador asigna la etiqueta correcta en el 80,25% de los casos.
        
        Otra forma de ver los resultados de nuestro clasificador es la matriz de confusión. La matriz de confusión es una tabla de doble entrada, donde un eje corresponde a la etiqueta real (y) y otro a la etiqueta predicha(pred_y). En la diagonal encontramos los aciertos, mientras que por fuera de la diagonal aquellas instancias mal clasificadas. Nuevamente, recomendamos ver la documentación.
        
        [ ]
        # creamos una matriz de confusión
        confusion_matrix(y, pred_y)		array([[407,17], [124, 166]])
                            -> Obtenemos unos datos que luego graficaremos
        
        
        Una forma más interesante de ver esta información es con la función plot_confusion_matrix:
        
        [ ]
        # creamos un gráfico para la matriz de confusión
        plot_confusion_matrix(arbol, X, y, cmap=plt.cm.Blues, values_format='.0f')		-> Armamos el gráfico con los datos anteriores
        
        
        O podemos obtener una versión normalizada (con valores entre 0 y 1, o dicho de otra manera, el porcentaje):
        
        [ ]
        # creamos un gráfico para la matriz de confusión normalizada
        plot_confusion_matrix(arbol, X, y, cmap=plt.cm.Blues, values_format='.2f', normalize='true')		-> Lo vemos en porcentaje.
        
        
        Podemos mostrar gráficamente el árbol de decisión que fue generado automáticamente al entrenar el modelo, para obtener un mayor conocimiento de nuestros datos y del funcionamiento del modelo. Las variables más importantes (aquellas que fueron de mayor utilidad para clasificar las diferentes instancias del dataset), aparecen en la parte superior, y en función de los valores asumidos, cada instancia será clasificada en diferentes ramas, tras lo cual el árbol se hace nuevamente una pregunta basada en los valores que asume esta instancia de una característica determinada.
        
        [ ]
        # mostramos un árbol gráficamente
        plt.figure(figsize= (10,8))
        tree.plot_tree(arbol, filled=True, feature_names=X.columns)
        plt.show()								-> Vemos el árboll de decisión.
                                            - la etiqueta gini del arbol de decisión es en relación al grado de pureza
                                            - Si agregamos el grado de profundidad del arbol máyor filtro y predicción.
        
        La rama de la izquierda representa el resultado verdadero (True), mientras que la rama derecha, representa el resultado falso (False). Te doy otra pista para interpretar el modelo: el color de cada rectángulo representa la etiqueta predicha por el modelo (en nuestro caso, la etiqueta azul representa el "sobrevive", o un valor de y=1, y la naranja, "no sobrevive", o un valor de y=0).
        
        A su vez, la tonalidad representa la seguridad que tiene el modelo en su predicción. A partir del entrenamiento, el modelo aprendió algunas reglas para clasificar las instancias de acuerdo a los valores asumidos por ciertas características. Dicha clasificación, sin embargo, contiene errores, dado que esta división puede generar que una proporción (cuanto menor, mejor), de instancias sean incorrectamente clasificadas, ya que en la realidad pertenecen a la otra categoría. La cantidad de instancias incorrectamente clasficadas se procesa matemáticamente en un indicador conocido como "impureza de Gini", el cual mide la cantidad de instacias incorrectamente clasificadas dentro de cada "hoja" del árbol. Alcanza el valor mínimo de cero cuando no hay instancias incorrectamente clasificadas. Esta información existe en nuestro gráfico de árbol, y a su vez determina el color de la hoja, siendo más intenso cuando menor es el valor de la "impureza de Gini", significando que la clasificación de esa hoja es más robusta para predecir correctamente un resultado.
        
        Podemos ver que nuestro modelo ha aprendido cosas muy interesantes:
        
        La primera pregunta que se hace nuestro modelo, es acerca del sexo de la persona: si es hombre (0) a continuación se pregunta su edad. Si es un hombre de edad 7 años o más, le asigna una etiqueta de "no sobrevive". Por el contrario, si es un niño de 6 años o menos, predecirá "sobrevive"
        El caso es diferente si como resultado de la primera pregunta, el valor de sexo fuera 1 (mujer). La pregunta que se hará a continuación el árbol es referido a qué clase pertenecía la pasajera: si fuera de 1° o 2° clase, le asignará la predicción "sobrevive", y si fuera de 3° clase, "no sobrevive".
        Lamentablemente no es un acontecimiento feliz, y tenemos los datos que demuestran que han existido pérdidas humanas. El modelo no tiene prejuicios al respecto, pero ha aprendido automáticamente de los datos para explicar cómo se dieron los sucesos. Piensa en cómo esto se relaciona con los acontecimientos históricos ocurridos en ocasión del hundimiento del Titanic. ¿Te suena la frase *"Mujeres y niños primero"*, al momento de lanzarse a los botes salvavidas? El modelo ha detectado que las mujeres tuvieron mayores oportunidades de supervivencia (y cuanto más pudientes, mejores serían sus chances), y que en el caso de los hombres, los niños pequeños tuvieron más suerte que los adolescentes o adultos.
        
        Otra visualización útil y más sintética que la anterior, es la posibilidad de graficar las importancias que han tenido cada una de las variables en la predicción obtenida. Esta importancia es dada por Scikit-Learn a cada feature (x) en función de qué tan útil ha sido para clasificar las instancias.
        
        [ ]
        # graficamos las importancias en un gráfico de barras
        # creamos las variables x (importancias) e y (columnas)
        importancias = arbol.feature_importances_
        columnas = X.columns
        
        
        # creamos el gráfico
        sns.barplot(columnas, importancias)
        plt.title('Importancia de cada Atributo')
        plt.show()				-> Vemos que el género, mujer es el factor más predominante al momento de predecir si sobrevive o no dicha persona.
                            -> Luego la clase, edad, hermEsp
                            -> Por lo tanto, las que mas sobrevivieron fueron mujeres de clase alta.
        Según el gráfico anterior, el factor más determinante fue el género, seguido de la clase del pasajero, y luego la edad. Complementado con el diagrama anterior, pudimos ver cómo las variables se influyeron mutuamente para determinar la posibilidad de supervivencia de acuerdo al género de la persona.
        
        El siguiente paso en el proceso de un científico de datos, sería el de optimizar su modelo de machine learning, para tratar de alcanzar una mayor precisión. Ten en cuenta que llegar a un 100% de precisión (accuracy) no es por lo general realista en ningún proyecto real, pero un umbral aceptable suele encontrarse entre los 85% y 95%, dependiendo de la complejidad del dataset y de los modelos ya disponibles (un modelo más complejo valdrá la pena únicamente si obtiene mejores resultados que un modelo simple).
        
        Experimenta con distintas profundidades y visualizar el árbol obtenidos con la función plot_tree del módulo tree de Scikit-Learn.
        Evalúa su desempeño calculando la exactitud y viendo su matriz de confusión.
        Observa la importancia asignada a cada atributo (feature_importances_). En la documentación encontrarás información que puede resultarte útil para mejorar los resultados obtenidos.
        ¿Te parece que lo obtenido concuerda con lo que esperabas?¿Qué más puedes aprender de la tragedia del Titanic viendo el árbol de decisíon y la importancia de cada atributo (feature)?                
    </pre>
    <div class="context">
        <a href="#arriba">Arriba</a>
    </div>
    <br>
    <br>

    
    <div class="context">
        <a href="../14/index.html">Volver</a>
        <a href="../../index.html">Inicio</a>
        <a href="../16/index.html">Siguiente</a>
    </div>

    <script src="js/01.js"></script>

</body>
</html>